{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pathlib\n",
    "import os\n",
    "import random\n",
    "from sklearn.utils import check_random_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/gurpreet.ag.singh/Desktop/Py/Success Tests/Original_db1k/'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = os.getcwd()+'/Original_db1k/'\n",
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(path+\"training_1000.nt\",sep = ' ', header = None)\n",
    "df_unseen = pd.read_csv(path+\"unseen_entities_1000.txt\",sep = ' ', header = None)\n",
    "df_val = pd.read_csv(path+\"validation_1000.nt\",sep = ' ', header = None)\n",
    "df_test = pd.read_csv(path+\"test_1000.nt\",sep = ' ', header = None)\n",
    "df_aux = pd.read_csv(path+\"auxiliary_1000.nt\",sep = ' ', header = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_train.shape (129743, 4)\n",
      "df_unseen.shape (1000, 1)\n",
      "df_val.shape (5000, 4)\n",
      "df_test.shape (1000, 4)\n",
      "df_aux.shape (10529, 4)\n"
     ]
    }
   ],
   "source": [
    "print(\"df_train.shape\",df_train.shape)\n",
    "print(\"df_unseen.shape\",df_unseen.shape)\n",
    "print(\"df_val.shape\",df_val.shape)\n",
    "print(\"df_test.shape\",df_test.shape)\n",
    "print(\"df_aux.shape\",df_aux.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "string = 'http://www.w3.org/1999/02/22-rdf-syntax-ns#type'\n",
    "df_train_s = df_train[df_train[1].str.contains(string)]\n",
    "df_aux_s = df_aux[df_aux[1].str.contains(string)]\n",
    "df_val_s = df_val[df_val[1].str.contains(string)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_train_s.shape (24779, 4)\n",
      "df_val_s.shape (1723, 4)\n",
      "df_aux_s.shape (4474, 4)\n"
     ]
    }
   ],
   "source": [
    "print(\"df_train_s.shape\",df_train_s.shape)\n",
    "print(\"df_val_s.shape\",df_val_s.shape)\n",
    "print(\"df_aux_s.shape\",df_aux_s.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a dataframe of non schema entities as df_ns\n",
    "df_train_ns = df_train.merge(df_train_s, indicator=True, how=\"left\")[lambda x: x._merge=='left_only'].drop('_merge',1)\n",
    "df_aux_ns = df_aux.merge(df_aux_s, indicator=True, how=\"left\")[lambda x: x._merge=='left_only'].drop('_merge',1)\n",
    "df_val_ns = df_val.merge(df_val_s, indicator=True, how=\"left\")[lambda x: x._merge=='left_only'].drop('_merge',1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_train_ns.shape (104964, 4)\n",
      "df_val_ns.shape (3277, 4)\n",
      "df_aux_ns.shape (6055, 4)\n"
     ]
    }
   ],
   "source": [
    "print(\"df_train_ns.shape\",df_train_ns.shape)\n",
    "print(\"df_val_ns.shape\",df_val_ns.shape)\n",
    "print(\"df_aux_ns.shape\",df_aux_ns.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Seralising now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mappings(X):\n",
    "    unique_ent = np.unique(np.concatenate((X[:, 0], X[:, 2])))\n",
    "    unique_rel = np.unique(X[:, 1])\n",
    "    ent_count = len(unique_ent)\n",
    "    rel_count = len(unique_rel)\n",
    "    rel_to_idx = dict(zip(unique_rel, range(rel_count)))\n",
    "    ent_to_idx = dict(zip(unique_ent, range(ent_count)))\n",
    "    return rel_to_idx, ent_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_idx(X, ent_to_idx=None, rel_to_idx=None):\n",
    "    x_idx_s = np.vectorize(ent_to_idx.get)(X[:, 0])\n",
    "    x_idx_p = np.vectorize(rel_to_idx.get)(X[:, 1])\n",
    "    x_idx_o = np.vectorize(ent_to_idx.get)(X[:, 2])\n",
    "\n",
    "    return np.dstack([x_idx_s, x_idx_p, x_idx_o]).reshape((-1, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ns = df_train_ns.values\n",
    "train_s = df_train_s.values\n",
    "aux_ns = df_aux_ns.values\n",
    "aux_s = df_aux_s.values\n",
    "val_ns = df_val_ns.values\n",
    "val_s = df_val_s.values\n",
    "unseen = df_unseen.values\n",
    "test = df_test.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unseen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = np.concatenate((train_ns,train_s,aux_ns,aux_s,val_ns,val_s,test))\n",
    "# vocab = np.concatenate((train_ns,train_s,val_ns,val_s,test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(146272, 4)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "rel_to_idx, ent_to_idx = create_mappings(vocab)\n",
    "train_ns = to_idx(train_ns, ent_to_idx, rel_to_idx)\n",
    "train_s = to_idx(train_s, ent_to_idx, rel_to_idx)\n",
    "aux_ns = to_idx(aux_ns, ent_to_idx, rel_to_idx)\n",
    "aux_s = to_idx(aux_s, ent_to_idx, rel_to_idx)\n",
    "val_ns = to_idx(val_ns, ent_to_idx, rel_to_idx)\n",
    "val_s = to_idx(val_s, ent_to_idx, rel_to_idx)\n",
    "# unseen = to_idx1(unseen, ent_to_idx, rel_to_idx)\n",
    "test = to_idx(test, ent_to_idx, rel_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of train_ns (104964, 3)\n",
      "shape of train_s (24779, 3)\n",
      "shape of val_ns (3277, 3)\n",
      "shape of val_s (1723, 3)\n",
      "shape of aux_ns (6055, 3)\n",
      "shape of aux_s (4474, 3)\n",
      "shape of test (1000, 3)\n"
     ]
    }
   ],
   "source": [
    "print(\"shape of train_ns\", train_ns.shape)\n",
    "print(\"shape of train_s\", train_s.shape)\n",
    "print(\"shape of val_ns\", val_ns.shape)\n",
    "print(\"shape of val_s\", val_s.shape)\n",
    "print(\"shape of aux_ns\", aux_ns.shape)\n",
    "print(\"shape of aux_s\", aux_s.shape)\n",
    "print(\"shape of test\", test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[50276,    32, 33012],\n",
       "       [37808,   279, 28351],\n",
       "       [74408,   305, 15365],\n",
       "       ...,\n",
       "       [32060,   293, 16957],\n",
       "       [40874,   359, 74561],\n",
       "       [23768,    32, 12779]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating corruptions for test (only) and validation(park) and labeling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_corruptions_for_fit(X, ent_to_idx=None, eta=1, rnd=None):\n",
    "    all_entities = list(ent_to_idx.values())\n",
    "    X_corr = []\n",
    "    for x in X:\n",
    "        for i in range(eta):\n",
    "            e = all_entities[rnd.randint(0, len(all_entities) - 1)]\n",
    "            if np.asscalar(rnd.rand(1, 1)) > 0.5:\n",
    "                X_corr.append([e, x[1], x[2]])\n",
    "            else:\n",
    "                X_corr.append([x[0], x[1], e])\n",
    "    return np.asarray(X_corr).reshape(-1, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[50276,    32, 33012],\n",
       "        [37808,   279, 28351],\n",
       "        [74408,   305, 15365],\n",
       "        ...,\n",
       "        [32060,   293, 16957],\n",
       "        [40874,   359, 74561],\n",
       "        [23768,    32, 12779]])]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_batches = np.array_split(test, 1)\n",
    "X_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[50276,    32, 14000],\n",
       "       [37808,   279, 22637],\n",
       "       [74408,   305, 55366],\n",
       "       ...,\n",
       "       [ 8269,   293, 16957],\n",
       "       [40874,   359, 35044],\n",
       "       [23768,    32, 49095]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neg_test = []\n",
    "rnd = check_random_state(50)\n",
    "for j in range(1):\n",
    "    X_neg_b = generate_corruptions_for_fit(X_batches[j], eta=1, rnd = rnd, ent_to_idx = ent_to_idx)\n",
    "    neg_test.append(X_neg_b)\n",
    "neg_test[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appending 0 to negative and 1 to positive set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(neg_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[50276,    32, 14000,     0],\n",
       "       [37808,   279, 22637,     0],\n",
       "       [74408,   305, 55366,     0],\n",
       "       ...,\n",
       "       [ 8269,   293, 16957,     0],\n",
       "       [40874,   359, 35044,     0],\n",
       "       [23768,    32, 49095,     0]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_zeros = np.zeros((1000,1), dtype=int)\n",
    "test_n = np.concatenate((neg_test[0],test_zeros),axis=1)\n",
    "test_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[50276,    32, 33012],\n",
       "        [37808,   279, 28351],\n",
       "        [74408,   305, 15365],\n",
       "        ...,\n",
       "        [32060,   293, 16957],\n",
       "        [40874,   359, 74561],\n",
       "        [23768,    32, 12779]])]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[50276,    32, 33012,     1],\n",
       "       [37808,   279, 28351,     1],\n",
       "       [74408,   305, 15365,     1],\n",
       "       ...,\n",
       "       [32060,   293, 16957,     1],\n",
       "       [40874,   359, 74561,     1],\n",
       "       [23768,    32, 12779,     1]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_ones = np.ones((1000,1), dtype=int)\n",
    "test_p = np.concatenate((X_batches[0],test_ones),axis=1)\n",
    "test_p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing it into file now (NOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_s = pd.DataFrame(train_s,index=None, columns=None)\n",
    "df_train_ns = pd.DataFrame(train_ns,index=None, columns=None)\n",
    "df_aux_s = pd.DataFrame(aux_s,index=None, columns=None)\n",
    "df_aux_ns = pd.DataFrame(aux_ns,index=None, columns=None)\n",
    "df_val_s = pd.DataFrame(val_s,index=None, columns=None)\n",
    "df_val_ns = pd.DataFrame(val_ns,index=None, columns=None)\n",
    "df_test_p = pd.DataFrame(test_p,index=None, columns=None)\n",
    "df_test_n = pd.DataFrame(test_n,index=None, columns=None)\n",
    "# df_test = pd.DataFrame(test,index=None, columns=None)\n",
    "# df_unseen = pd.DataFrame(unseen,index=None, columns=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of train_ns (104964, 3)\n",
      "shape of train_s (24779, 3)\n",
      "shape of val_ns (3277, 3)\n",
      "shape of val_s (1723, 3)\n",
      "shape of aux_ns (6055, 3)\n",
      "shape of aux_s (4474, 3)\n"
     ]
    }
   ],
   "source": [
    "print(\"shape of train_ns\", df_train_ns.shape)\n",
    "print(\"shape of train_s\", df_train_s.shape)\n",
    "print(\"shape of val_ns\", df_val_ns.shape)\n",
    "print(\"shape of val_s\", df_val_s.shape)\n",
    "print(\"shape of aux_ns\", df_aux_ns.shape)\n",
    "print(\"shape of aux_s\", df_aux_s.shape)\n",
    "# print(\"shape of test\", test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining the train + validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_s = df_train_s.append(df_val_s)\n",
    "df_train_ns = df_train_ns.append(df_val_ns)\n",
    "df_test = df_test_p.append(df_test_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A New Directory is created, named: 'comb'\n"
     ]
    }
   ],
   "source": [
    "path = os.getcwd()\n",
    "pathlib.Path(path+'/comb').mkdir(parents=True, exist_ok=True)\n",
    "print(\"A New Directory is created, named: 'comb'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To write with train + validate\n",
    "\n",
    "# path = os.getcwd()+'/comb/'\n",
    "# df_train_s.to_csv(path+'train_s',sep='\\t', header=False, index=False, encoding = \"utf-8\")\n",
    "# df_train_ns.to_csv(path+'train_ns',sep='\\t', header=False, index=False, encoding = \"utf-8\")\n",
    "\n",
    "# # NOT USING NOW\n",
    "# # df_aux_s.to_csv(path+'aux_s',sep='\\t', header=False, index=False, encoding = \"utf-8\")\n",
    "# # df_aux_ns.to_csv(path+'aux_ns',sep='\\t', header=False, index=False, encoding = \"utf-8\")\n",
    "# # df_val_s.to_csv(path+'val_s',sep='\\t', header=False, index=False, encoding = \"utf-8\")\n",
    "# # df_val_ns.to_csv(path+'val_ns',sep='\\t', header=False, index=False, encoding = \"utf-8\")\n",
    "\n",
    "# df_test.to_csv(path+'test',sep='\\t', header=False, index=False, encoding = \"utf-8\")\n",
    "# # df_unseen.to_csv(path+'unseen',sep='\\t', header=False, index=False, encoding = \"utf-8\")\n",
    "\n",
    "# print(\"All the seralised files are now saved inside comb folder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining train + val + aux = train_all_ns and train_all_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_s = df_train_s.append(df_aux_s)\n",
    "df_train_ns = df_train_ns.append(df_aux_ns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of train_ns all combined (114296, 3)\n",
      "shape of train_s all combined (30976, 3)\n",
      "shape of val_ns old (3277, 3)\n",
      "shape of val_s old (1723, 3)\n",
      "shape of aux_ns old (6055, 3)\n",
      "shape of aux_s old (4474, 3)\n"
     ]
    }
   ],
   "source": [
    "print(\"shape of train_ns all combined\", df_train_ns.shape)\n",
    "print(\"shape of train_s all combined\", df_train_s.shape)\n",
    "print(\"shape of val_ns old\", df_val_ns.shape)\n",
    "print(\"shape of val_s old\", df_val_s.shape)\n",
    "print(\"shape of aux_ns old\", df_aux_ns.shape)\n",
    "print(\"shape of aux_s old\", df_aux_s.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For non schema 114296\n",
      "For Schema 30976\n"
     ]
    }
   ],
   "source": [
    "print(\"For non schema\",104964 + 3277 + 6055)\n",
    "print(\"For Schema\",24779 + 1723 + 4474)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Making a complete set of Train:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(145272, 3)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_all = df_train_ns.append(df_train_s)\n",
    "df_train_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All the seralised files are now saved inside comb folder\n"
     ]
    }
   ],
   "source": [
    "path = os.getcwd()+'/comb/'\n",
    "df_train_s.to_csv(path+'train_s',sep='\\t', header=False, index=False, encoding = \"utf-8\")\n",
    "df_train_ns.to_csv(path+'train_ns',sep='\\t', header=False, index=False, encoding = \"utf-8\")\n",
    "df_test.to_csv(path+'test',sep='\\t', header=False, index=False, encoding = \"utf-8\")\n",
    "df_train_all.to_csv(path+'train_all',sep='\\t', header=False, index=False, encoding = \"utf-8\")\n",
    "print(\"All the seralised files are now saved inside comb folder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/gurpreet.ag.singh/Desktop/Py/Success Tests'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
